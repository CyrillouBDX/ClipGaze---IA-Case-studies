{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8dd6188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:01:38.026654Z",
     "start_time": "2025-11-28T20:01:38.001229Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import clip\n",
    "from models import Transformer\n",
    "from CLIPGaze import CLIPGaze\n",
    "from feature_extractor import visual_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94b0f6f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:01:43.014391Z",
     "start_time": "2025-11-28T20:01:42.998366Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(model, src, task, device = \"cuda:0\", im_h=20, im_w=32, project_num = 16, num_samples = 1):\n",
    "    task = torch.tensor(task.astype(np.float32)).to(device).unsqueeze(0).repeat(num_samples, 1)\n",
    "    firstfix = torch.tensor([(im_h//2)*project_num, (im_w//2)*project_num]).unsqueeze(0).repeat(num_samples, 1)\n",
    "    with torch.no_grad():\n",
    "        token_prob, ys, xs, ts = model(src = src, tgt = firstfix, task = task)\n",
    "    token_prob = token_prob.detach().cpu().numpy()\n",
    "    ys = ys.cpu().detach().numpy()\n",
    "    xs = xs.cpu().detach().numpy()\n",
    "    ts = ts.cpu().detach().numpy()\n",
    "    scanpaths = []\n",
    "    for i in range(num_samples):\n",
    "        ys_i = [(im_h//2) * project_num] + list(ys[:, i, 0])[1:]\n",
    "        xs_i = [(im_w//2) * project_num] + list(xs[:, i, 0])[1:]\n",
    "        ts_i = list(ts[:, i, 0])\n",
    "        token_type = [0] + list(np.argmax(token_prob[:, i, :], axis=-1))[1:]\n",
    "        scanpath = []\n",
    "        for tok, y, x, t in zip(token_type, ys_i, xs_i, ts_i):\n",
    "            if tok == 0:\n",
    "                scanpath.append([min(im_h * project_num - 2, y),min(im_w * project_num - 2, x), t])\n",
    "            else:\n",
    "                break\n",
    "        scanpaths.append(np.array(scanpath))\n",
    "    return scanpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e64df245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:01:43.957749Z",
     "start_time": "2025-11-28T20:01:43.943977Z"
    }
   },
   "outputs": [],
   "source": [
    "def postprocessScanpaths(trajs):\n",
    "    # convert actions to scanpaths\n",
    "    scanpaths = []\n",
    "    for traj in trajs:\n",
    "        task_name, img_name, condition, subject, fixs = traj\n",
    "        scanpaths.append({\n",
    "            'X': fixs[:, 1],\n",
    "            'Y': fixs[:, 0],\n",
    "            'T': fixs[:, 2],\n",
    "            'subject':subject,\n",
    "            'name': img_name,\n",
    "            'task': task_name,\n",
    "            'condition': condition\n",
    "        })\n",
    "    return scanpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83b3873916ad2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clipgaze_model(checkpoint_path, device, im_hw=(20, 32), max_len=7, hidden_dim=1024, nhead=8, num_decoder=6):\n",
    "    \"\"\"Instantiate CLIPGaze and restore pretrained weights.\"\"\"\n",
    "    transformer = Transformer(\n",
    "        nhead=nhead,\n",
    "        d_model=hidden_dim,\n",
    "        num_decoder_layers=num_decoder,\n",
    "        dim_feedforward=hidden_dim,\n",
    "        device=device,\n",
    "        im_h=im_hw[0],\n",
    "        im_w=im_hw[1],\n",
    "    ).to(device)\n",
    "    model = CLIPGaze(transformer, spatial_dim=im_hw, max_len=max_len, device=device).to(device)\n",
    "    state = torch.load(checkpoint_path, map_location=device)\n",
    "    if isinstance(state, dict) and \"model\" in state:\n",
    "        state = state[\"model\"]\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def extract_visual_features(image_path, clip_visual, device, token_shape, extract_layers=(6, 12, 18)):\n",
    "    \"\"\"Run CLIP visual encoder and format activations the way CLIPGaze expects.\"\"\"\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.Resize((280, 448)),\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_dtype = clip_visual.conv1.weight.dtype\n",
    "    image_tensor = preprocess(image).unsqueeze(0).to(device=device, dtype=img_dtype)\n",
    "    with torch.no_grad():\n",
    "        _, activations, _ = visual_forward(\n",
    "            clip_visual,\n",
    "            image_tensor,\n",
    "            extract_layers=extract_layers,\n",
    "            token_shape=token_shape,\n",
    "        )\n",
    "    activations = [x.permute(1, 0, 2).to(device=device, dtype=torch.float32) for x in activations]\n",
    "    return [activations]\n",
    "\n",
    "\n",
    "def encode_target_prompt(prompt, clip_model, device):\n",
    "    text_tokens = clip.tokenize(prompt).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens).squeeze(0)\n",
    "    return text_features.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "def plot_scanpaths(image_path, scanpaths, im_hw=(20, 32), project_num=16, duration_radius=(5, 18)):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_w, img_h = image.size\n",
    "    scale_x = img_w / (im_hw[1] * project_num)\n",
    "    scale_y = img_h / (im_hw[0] * project_num)\n",
    "    colors = [\"#ff6b6b\", \"#4ecdc4\", \"#ffa62b\", \"#5d5fef\", \"#2ec4b6\"]\n",
    "    num_samples = len(scanpaths)\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(5 * num_samples, 5))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    for idx, (ax, scanpath) in enumerate(zip(axes, scanpaths)):\n",
    "        ax.imshow(image)\n",
    "        if len(scanpath) > 0:\n",
    "            xs = scanpath[:, 1] * scale_x\n",
    "            ys = scanpath[:, 0] * scale_y\n",
    "            durations = scanpath[:, 2]\n",
    "            color = colors[idx % len(colors)]\n",
    "            ax.plot(xs, ys, \"-\", color=color, linewidth=2)\n",
    "            dur_min, dur_max = durations.min(), durations.max()\n",
    "            if dur_max == dur_min:\n",
    "                radii = np.full_like(durations, duration_radius[0], dtype=float)\n",
    "            else:\n",
    "                radii = np.interp(durations, (dur_min, dur_max), duration_radius)\n",
    "            # Use scatter so marker radius grows with fixation duration\n",
    "            ax.scatter(\n",
    "                xs,\n",
    "                ys,\n",
    "                s=np.square(radii),\n",
    "                color=color,\n",
    "                edgecolors=\"white\",\n",
    "                linewidths=0.8,\n",
    "                zorder=3,\n",
    "            )\n",
    "            for step, (x, y) in enumerate(zip(xs, ys), start=1):\n",
    "                ax.text(x + 2, y + 2, str(step), color=\"white\", fontsize=8, weight=\"bold\")\n",
    "        ax.set_title(f\"Sample {idx + 1} ({len(scanpath)} fix)\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    for idx, (ax, scanpath) in enumerate(zip(axes, scanpaths)):\n",
    "        ax.imshow(image)\n",
    "        if len(scanpath) > 0:\n",
    "            xs = scanpath[:, 1] * scale_x\n",
    "            ys = scanpath[:, 0] * scale_y\n",
    "            durations = scanpath[:, 2]\n",
    "            color = colors[idx % len(colors)]\n",
    "            ax.plot(xs, ys, \"-\", color=color, linewidth=2)\n",
    "            dur_min, dur_max = durations.min(), durations.max()\n",
    "            if dur_max == dur_min:\n",
    "                radii = np.full_like(durations, duration_radius[0], dtype=float)\n",
    "            else:\n",
    "                radii = np.interp(durations, (dur_min, dur_max), duration_radius)\n",
    "\n",
    "            ax.scatter(xs, ys, s=np.square(radii), color=color, edgecolors=\"white\", linewidths=0.8, zorder=3)\n",
    "\n",
    "            for step, (x, y) in enumerate(zip(xs, ys), start=1):\n",
    "                ax.text(x + 2, y + 2, str(step), color=\"white\", fontsize=8, weight=\"bold\")\n",
    "\n",
    "        ax.set_title(f\"Sample {idx + 1}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Sauvegarde sur le disque\n",
    "    plt.savefig(output_path, dpi=150)\n",
    "    # Ferme la figure pour libérer la mémoire (très important dans une boucle)\n",
    "    plt.close(fig)\n",
    "\n",
    "def save_scanpath_result(image_path, scanpaths, output_path, im_hw=(20, 32), project_num=16, duration_radius=(5, 18)):\n",
    "    # Similaire à plot_scanpaths mais sauvegarde le fichier\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_w, img_h = image.size\n",
    "    scale_x = img_w / (im_hw[1] * project_num)\n",
    "    scale_y = img_h / (im_hw[0] * project_num)\n",
    "    colors = [\"#ff6b6b\", \"#4ecdc4\", \"#ffa62b\", \"#5d5fef\", \"#2ec4b6\"]\n",
    "\n",
    "    num_samples = len(scanpaths)\n",
    "    # On crée la figure sans l'afficher\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(5 * num_samples, 5))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, (ax, scanpath) in enumerate(zip(axes, scanpaths)):\n",
    "        ax.imshow(image)\n",
    "        if len(scanpath) > 0:\n",
    "            xs = scanpath[:, 1] * scale_x\n",
    "            ys = scanpath[:, 0] * scale_y\n",
    "            durations = scanpath[:, 2]\n",
    "            color = colors[idx % len(colors)]\n",
    "            ax.plot(xs, ys, \"-\", color=color, linewidth=2)\n",
    "            dur_min, dur_max = durations.min(), durations.max()\n",
    "            if dur_max == dur_min:\n",
    "                radii = np.full_like(durations, duration_radius[0], dtype=float)\n",
    "            else:\n",
    "                radii = np.interp(durations, (dur_min, dur_max), duration_radius)\n",
    "\n",
    "            ax.scatter(xs, ys, s=np.square(radii), color=color, edgecolors=\"white\", linewidths=0.8, zorder=3)\n",
    "\n",
    "            for step, (x, y) in enumerate(zip(xs, ys), start=1):\n",
    "                ax.text(x + 2, y + 2, str(step), color=\"white\", fontsize=8, weight=\"bold\")\n",
    "\n",
    "        ax.set_title(f\"Sample {idx + 1}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Sauvegarde sur le disque\n",
    "    plt.savefig(output_path, dpi=150)\n",
    "    # Ferme la figure pour libérer la mémoire (très important dans une boucle)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4654d677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:01:54.917855Z",
     "start_time": "2025-11-28T20:01:46.127398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle...\n",
      "Modèle chargé.\n",
      "8 images trouvées à traiter.\n",
      "[1/8] Traitement de : appel.png ... Terminé -> appel\n",
      "[2/8] Traitement de : apple.png ... Terminé -> apple\n",
      "[3/8] Traitement de : camera.png ... Terminé -> camera\n",
      "[4/8] Traitement de : chair.png ... Terminé -> chair\n",
      "[5/8] Traitement de : duck.png ... Terminé -> duck\n",
      "[6/8] Traitement de : green apple.png ... Terminé -> green apple\n",
      "[7/8] Traitement de : red chair.png ... Terminé -> red chair\n",
      "[8/8] Traitement de : table.png ... Terminé -> table\n",
      "Traitement terminé ! Voir le dossier: ./results/renders\n"
     ]
    }
   ],
   "source": [
    "# --- Inference configuration ---\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Dossier avec les images\n",
    "input_folder = \"./images_renders\"\n",
    "# Dossier où les résultats seront sauvegardés\n",
    "output_folder = \"./results/renders\"\n",
    "# Extensions d'images à chercher\n",
    "extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n",
    "\n",
    "# Paramètres du modèle (inchangés)\n",
    "checkpoint_path = Path(\"CLIPGaze_TP.pkg\")\n",
    "im_hw = (20, 32)\n",
    "project_num = 16\n",
    "num_samples = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_version = \"ViT-L/14@336px\"\n",
    "token_shape_lookup = {\"ViT-B/32\": (7, 7), \"ViT-B/16\": (14, 14), \"ViT-L/14@336px\": (24, 24)}\n",
    "\n",
    "# Création du dossier de sortie s'il n'existe pas\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 1. Chargement du modèle (une seule fois pour gagner du temps)\n",
    "print(\"Chargement du modèle...\")\n",
    "token_shape = token_shape_lookup[clip_version]\n",
    "clip_model, _ = clip.load(clip_version, device=device, jit=False)\n",
    "clip_model.eval()\n",
    "model = load_clipgaze_model(checkpoint_path, device, im_hw=im_hw)\n",
    "print(\"Modèle chargé.\")\n",
    "\n",
    "# 2. Récupération de la liste des images\n",
    "image_files = []\n",
    "for ext in extensions:\n",
    "    image_files.extend(list(Path(input_folder).glob(ext)))\n",
    "image_files = sorted(image_files)\n",
    "\n",
    "print(f\"{len(image_files)} images trouvées à traiter.\")\n",
    "\n",
    "# 3. Boucle de traitement\n",
    "for i, img_path in enumerate(image_files):\n",
    "    try:\n",
    "        filename = img_path.name\n",
    "        print(f\"[{i+1}/{len(image_files)}] Traitement de : {filename} ...\", end=\" \")\n",
    "\n",
    "        # Déduction du mot cible depuis le nom de fichier (ex: 'oven_01.jpg' -> 'oven')\n",
    "        target_word = img_path.stem.split(\"_\")[0]\n",
    "        target_prompt = target_word.replace(\"-\", \" \")\n",
    "\n",
    "        # Encodage du texte (Prompt)\n",
    "        task_embedding = encode_target_prompt(target_prompt, clip_model, device)\n",
    "\n",
    "        # Extraction des features de l'image\n",
    "        image_features = extract_visual_features(str(img_path), clip_model.visual, device, token_shape)\n",
    "\n",
    "        # Exécution du modèle (Génération des scanpaths)\n",
    "        scanpaths = run_model(\n",
    "            model=model,\n",
    "            src=image_features,\n",
    "            task=task_embedding,\n",
    "            device=device,\n",
    "            im_h=im_hw[0],\n",
    "            im_w=im_hw[1],\n",
    "            project_num=project_num,\n",
    "            num_samples=num_samples,\n",
    "        )\n",
    "\n",
    "        # Sauvegarde du résultat visuel\n",
    "        save_path = Path(output_folder) / f\"result_{img_path.stem}.png\"\n",
    "        save_scanpath_result(\n",
    "            str(img_path),\n",
    "            scanpaths,\n",
    "            str(save_path),\n",
    "            im_hw=im_hw,\n",
    "            project_num=project_num\n",
    "        )\n",
    "\n",
    "        print(f\"Terminé -> {target_word}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERREUR sur {img_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"Traitement terminé ! Voir le dossier:\", output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc563c1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T08:41:06.228963Z",
     "start_time": "2025-12-15T08:41:06.214936Z"
    }
   },
   "outputs": [],
   "source": [
    "# target_prompt = target_word.replace(\"-\", \" \")\n",
    "# task_embedding = encode_target_prompt(target_prompt, clip_model, device)\n",
    "#\n",
    "# image_features = extract_visual_features(image_path, clip_model.visual, device, token_shape)\n",
    "#\n",
    "# scanpaths = run_model(\n",
    "#     model=model,\n",
    "#     src=image_features,\n",
    "#     task=task_embedding,\n",
    "#     device=device,\n",
    "#     im_h=im_hw[0],\n",
    "#     im_w=im_hw[1],\n",
    "#     project_num=project_num,\n",
    "#     num_samples=num_samples,\n",
    "#  )\n",
    "#\n",
    "# print(f\"Target prompt: '{target_prompt}' | Num samples: {num_samples}\")\n",
    "# plot_scanpaths(image_path, scanpaths, im_hw=im_hw, project_num=project_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956aacd7f341dbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ClipGazeCyril",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
